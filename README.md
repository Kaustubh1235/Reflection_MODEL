# ğŸ›ï¸ Legal QA System - Complete Architecture Deep Dive & README

## ğŸ“‹ Table of Contents
1. [System Architecture Overview](#system-architecture-overview)
2. [Component Deep Dive](#component-deep-dive)
3. [Data Flow](#data-flow)
4. [Technical Implementation](#technical-implementation)
5. [README.md for Showcase](#readme-for-showcase)

---

# ğŸ¯ SYSTEM ARCHITECTURE OVERVIEW

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEGAL QA ADVISORY SYSTEM                      â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   RAW      â”‚â”€â”€â”€â–¶â”‚  PRE-      â”‚â”€â”€â”€â–¶â”‚   TRAINING      â”‚       â”‚
â”‚  â”‚   DATA     â”‚    â”‚  PROCESSOR â”‚    â”‚   PIPELINE      â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                              â”‚                   â”‚
â”‚                                              â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           HYBRID INFERENCE ENGINE                    â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚       â”‚
â”‚  â”‚  â”‚  GROQ    â”‚â”€â–¶â”‚   T5     â”‚â”€â–¶â”‚  MEMORY SYSTEM   â”‚ â”‚       â”‚
â”‚  â”‚  â”‚   LLM    â”‚  â”‚ CRITIQUE â”‚  â”‚  (SQLite + Stats)â”‚ â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚    INTERACTIVE LEGAL ADVISORY INTERFACE    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ” COMPONENT DEEP DIVE

## 1. DATA PREPROCESSING LAYER

### 1.1 Raw Data Formatter (`json_to_csv_formatter.py`)

**Purpose**: Convert raw JSON legal data to standardized CSV format

```python
Input:  constitution_qa.json
        {
          "question": "What are fundamental rights?",
          "answer": "Fundamental rights include..."
        }

Process: 
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Load JSON           â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Generate Tags       â”‚
        â”‚ [what-are-fundamental]â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Format Question     â”‚
        â”‚ Add Tag Prefix      â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
Output: â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ formatted_qa_data.csvâ”‚
        â”‚ "[ tag ] question","answer"â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- Generates semantic tags from question content
- Removes special characters
- Handles missing data
- CSV export with proper quoting

**Code Flow**:
```python
1. Load JSON â†’ pd.read_json()
2. Extract question words â†’ regex: r'\b\w+\b'
3. Create tag â†’ first N words joined by "-"
4. Format: "[ tag ] original_question"
5. Export â†’ to_csv() with quoting=1
```

---

### 1.2 Enhanced Preprocessor (`preprocess.py`)

**Purpose**: Advanced NLP preprocessing for training data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PREPROCESSING PIPELINE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Raw CSV                                                  â”‚
â”‚     â”‚                                                     â”‚
â”‚     â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 1. DATA VALIDATION  â”‚                                â”‚
â”‚  â”‚   - Check columns   â”‚                                â”‚
â”‚  â”‚   - Handle encoding â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 2. QUALITY ASSESS   â”‚                                â”‚
â”‚  â”‚   - Missing values  â”‚                                â”‚
â”‚  â”‚   - Duplicates      â”‚                                â”‚
â”‚  â”‚   - Text stats      â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 3. TEXT CLEANING    â”‚                                â”‚
â”‚  â”‚   - Lowercase       â”‚                                â”‚
â”‚  â”‚   - Remove URLs     â”‚                                â”‚
â”‚  â”‚   - Remove HTML     â”‚                                â”‚
â”‚  â”‚   - Lemmatization   â”‚                                â”‚
â”‚  â”‚   - Stopword remove â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 4. TOKENIZATION     â”‚                                â”‚
â”‚  â”‚   - Word tokenize   â”‚                                â”‚
â”‚  â”‚   - Filter by lengthâ”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 5. DUPLICATE HANDLE â”‚                                â”‚
â”‚  â”‚   - TF-IDF vectors  â”‚                                â”‚
â”‚  â”‚   - Cosine sim      â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ 6. TRAIN/TEST SPLIT â”‚                                â”‚
â”‚  â”‚   - Stratified      â”‚                                â”‚
â”‚  â”‚   - 70/20/10        â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚        â”‚                                                  â”‚
â”‚        â–¼                                                  â”‚
â”‚  Clean Datasets                                          â”‚
â”‚  (train.csv, test.csv, validation.csv)                  â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Advanced NLP Techniques Used**:

```python
# 1. LEMMATIZATION (Word to Base Form)
"running" â†’ "run"
"studies" â†’ "study"
"constitutional" â†’ "constitutional"

# 2. POS TAGGING (Part of Speech)
"The Supreme Court ruled" 
â†’ [('The', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('ruled', 'VBD')]

# 3. STOPWORD REMOVAL
"What are the fundamental rights under the Constitution?"
â†’ "fundamental rights Constitution"

# 4. TECHNICAL TERM PRESERVATION
Preserved: "Due Process", "Equal Protection", "Judicial Review"
Not Removed: Legal terminology kept intact

# 5. DUPLICATE DETECTION
Method: TF-IDF + Cosine Similarity
Threshold: 0.85 similarity = duplicate
```

**Configuration Class**:
```python
@dataclass
class PreprocessingConfig:
    # Data paths
    input_csv_path: str
    text_column: str
    target_column: Optional[str] = None
    output_dir: str = "processed_data"
    
    # Cleaning options
    remove_urls: bool = True
    remove_emails: bool = True
    remove_html: bool = True
    remove_numbers: bool = True
    remove_punctuation: bool = True
    remove_stopwords: bool = True
    lemmatize: bool = True
    min_word_length: int = 2
    max_text_length: Optional[int] = None
    
    # Advanced options
    custom_stopwords: List[str] = None
    preserve_patterns: List[str] = None
    handle_duplicates: str = "remove"
    min_samples_per_class: int = 2
```

---

## 2. TRAINING PIPELINE LAYER

### 2.1 Dataset Architecture

```python
class LegalQADataset(Dataset):
    """
    PyTorch Dataset for Legal Q&A
    
    Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Raw CSV Data       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  __getitem__(idx)   â”‚
    â”‚  - Get Q&A pair     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Format Prompt      â”‚
    â”‚  "Legal Question: {q}â”‚
    â”‚   Provide answer:"  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Tokenize           â”‚
    â”‚  - Input IDs        â”‚
    â”‚  - Attention Mask   â”‚
    â”‚  - Labels           â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Pad/Truncate       â”‚
    â”‚  Max: 768 tokens    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Return Dict        â”‚
    â”‚  {input_ids,        â”‚
    â”‚   attention_mask,   â”‚
    â”‚   labels}           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
```

**Key Implementation Details**:

```python
def __getitem__(self, idx):
    # 1. Get row
    row = self.df.iloc[idx]
    question = str(row["question_clean"]).strip()
    answer = str(row["answer_clean"]).strip()
    
    # 2. Enhanced prompt for legal context
    input_text = f"Legal Question: {question}\n\nProvide a comprehensive legal answer:"
    target_text = answer
    
    # 3. Tokenize input
    input_encoding = self.tokenizer(
        input_text,
        max_length=768,  # Longer for legal text
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    
    # 4. Tokenize target
    target_encoding = self.tokenizer(
        target_text,
        max_length=768,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    
    # 5. Prepare labels (mask padding tokens)
    labels = target_encoding.input_ids.squeeze()
    labels[labels == self.tokenizer.pad_token_id] = -100
    
    return {
        "input_ids": input_encoding.input_ids.squeeze(),
        "attention_mask": input_encoding.attention_mask.squeeze(),
        "labels": labels
    }
```

---

### 2.2 Training Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     T5 TRAINING PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Pretrained Flan-T5-Base (248M parameters)                     â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ Add Legal Tokens    â”‚                                       â”‚
â”‚  â”‚ <legal_critique>    â”‚                                       â”‚
â”‚  â”‚ <legal_improve>     â”‚                                       â”‚
â”‚  â”‚ <statute>           â”‚                                       â”‚
â”‚  â”‚ <case_law>          â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚        â”‚                                                        â”‚
â”‚        â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ Resize Embeddings   â”‚                                       â”‚
â”‚  â”‚ 32128 â†’ 32135       â”‚                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚        â”‚                                                        â”‚
â”‚        â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚     TRAINING LOOP                   â”‚                      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
â”‚  â”‚  â”‚ Epoch 1                       â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚  â”œâ”€ Batch 1 (Forward pass)    â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚  â”œâ”€ Loss calculation          â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚  â”œâ”€ Backward pass             â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚  â””â”€ Weight update             â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚                               â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚ Epoch 2-5 (repeated)          â”‚  â”‚                      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚  â”‚                                      â”‚                      â”‚
â”‚  â”‚  Checkpoint Saving:                 â”‚                      â”‚
â”‚  â”‚  - Every 200 steps                  â”‚                      â”‚
â”‚  â”‚  - Keep best 3 models               â”‚                      â”‚
â”‚  â”‚  - Based on eval_loss               â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚        â”‚                                                        â”‚
â”‚        â–¼                                                        â”‚
â”‚  Fine-tuned Legal T5 Model                                     â”‚
â”‚  - Specialized for constitutional law                          â”‚
â”‚  - Can critique and improve answers                            â”‚
â”‚  - Understands legal terminology                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training Configuration**:

```python
TrainingArguments(
    output_dir=MODEL_DIR,
    num_train_epochs=5,              # More epochs for legal domain
    per_device_train_batch_size=2,   # Small batch for CPU
    per_device_eval_batch_size=2,
    
    # Learning rate schedule
    warmup_steps=100,
    learning_rate=5e-5,              # Standard for T5
    weight_decay=0.01,
    
    # Evaluation strategy
    evaluation_strategy="steps",
    eval_steps=200,
    
    # Checkpoint management
    save_steps=200,
    save_total_limit=3,              # Keep only best 3
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    
    # Optimization
    dataloader_num_workers=0,        # CPU optimization
    dataloader_pin_memory=False,
    
    # Logging
    logging_steps=50,
    logging_dir=f"{MODEL_DIR}/logs",
    report_to=None                   # Disable wandb
)
```

---

## 3. INFERENCE ENGINE (RUNTIME)

### 3.1 Hybrid Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HYBRID INFERENCE PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  User Question: "What are fundamental rights?"                   â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ 1. MEMORY CHECK     â”‚                                        â”‚
â”‚  â”‚                     â”‚                                        â”‚
â”‚  â”‚ Similarity Search   â”‚                                        â”‚
â”‚  â”‚ in SQLite DB        â”‚                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚        â”‚                                                         â”‚
â”‚        â”œâ”€â”€â”€ Similar Found (â‰¥0.65) â”€â”€â”€â”                         â”‚
â”‚        â”‚                               â”‚                         â”‚
â”‚        â”‚                               â–¼                         â”‚
â”‚        â”‚                        Return Cached                    â”‚
â”‚        â”‚                         Answer                          â”‚
â”‚        â”‚                                                         â”‚
â”‚        â””â”€â”€â”€ Not Found â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                                                 â”‚                â”‚
â”‚                                                 â–¼                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ 2. GROQ LLM GENERATION                        â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  System Prompt:                               â”‚       â”‚
â”‚         â”‚  "You are a knowledgeable legal advisor...   â”‚       â”‚
â”‚         â”‚   Provide accurate legal information...       â”‚       â”‚
â”‚         â”‚   Always include disclaimers..."              â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  User Prompt:                                 â”‚       â”‚
â”‚         â”‚  "Legal Question: {question}"                 â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  âš¡ Fast generation (< 2 seconds)            â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         Initial Answer (GROQ Output)                            â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ 3. T5 CRITIQUE PHASE                          â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  Prompt Template:                             â”‚       â”‚
â”‚         â”‚  "legal_critique: Analyze this legal         â”‚       â”‚
â”‚         â”‚   response for accuracy, completeness,        â”‚       â”‚
â”‚         â”‚   and potential issues..."                    â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  Input: Question + GROQ Answer                â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  T5 generates:                                â”‚       â”‚
â”‚         â”‚  - Identified errors                          â”‚       â”‚
â”‚         â”‚  - Missing information                        â”‚       â”‚
â”‚         â”‚  - Legal accuracy assessment                  â”‚       â”‚
â”‚         â”‚  - Constitutional references needed           â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         Critique/Reflection                                     â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ 4. T5 IMPROVEMENT PHASE                       â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  Prompt Template:                             â”‚       â”‚
â”‚         â”‚  "legal_improve: Using the critique,         â”‚       â”‚
â”‚         â”‚   provide comprehensive improved response..." â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  Input:                                       â”‚       â”‚
â”‚         â”‚  - Original Question                          â”‚       â”‚
â”‚         â”‚  - GROQ Answer                                â”‚       â”‚
â”‚         â”‚  - T5 Critique                                â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  T5 generates:                                â”‚       â”‚
â”‚         â”‚  - Fixed errors                               â”‚       â”‚
â”‚         â”‚  - Added missing info                         â”‚       â”‚
â”‚         â”‚  - Better structure                           â”‚       â”‚
â”‚         â”‚  - Legal citations                            â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         Enhanced Legal Answer                                   â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ 5. MEMORY STORAGE                             â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  Store to SQLite:                             â”‚       â”‚
â”‚         â”‚  - Question (+ hash)                          â”‚       â”‚
â”‚         â”‚  - GROQ answer                                â”‚       â”‚
â”‚         â”‚  - T5 critique                                â”‚       â”‚
â”‚         â”‚  - Improved answer                            â”‚       â”‚
â”‚         â”‚  - Confidence score                           â”‚       â”‚
â”‚         â”‚  - Timestamp                                  â”‚       â”‚
â”‚         â”‚  - Source: "auto-reflection"                  â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚ 6. USER FEEDBACK LOOP                         â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  User can:                                    â”‚       â”‚
â”‚         â”‚  - Accept answer (y)                          â”‚       â”‚
â”‚         â”‚  - Reject and provide correction (n)          â”‚       â”‚
â”‚         â”‚  - Direct correction input                    â”‚       â”‚
â”‚         â”‚                                                â”‚       â”‚
â”‚         â”‚  If corrected:                                â”‚       â”‚
â”‚         â”‚  - Store as "user-correction"                 â”‚       â”‚
â”‚         â”‚  - Confidence = 1.0                           â”‚       â”‚
â”‚         â”‚  - Flag for retraining                        â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.2 Memory System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MEMORY SYSTEM (SQLite)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  FEEDBACK TABLE                             â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ id                 INTEGER PRIMARY KEY                      â”‚ â”‚
â”‚  â”‚ question           TEXT NOT NULL                            â”‚ â”‚
â”‚  â”‚ question_hash      TEXT NOT NULL (MD5[:8])                 â”‚ â”‚
â”‚  â”‚ groq_answer        TEXT NOT NULL                            â”‚ â”‚
â”‚  â”‚ improvement        TEXT NOT NULL                            â”‚ â”‚
â”‚  â”‚ t5_reflection      TEXT NOT NULL                            â”‚ â”‚
â”‚  â”‚ source             TEXT (auto-reflection/user-correction)  â”‚ â”‚
â”‚  â”‚ confidence_score   REAL (0.0 - 1.0)                        â”‚ â”‚
â”‚  â”‚ improvement_type   TEXT                                     â”‚ â”‚
â”‚  â”‚ timestamp          TEXT (ISO format)                        â”‚ â”‚
â”‚  â”‚ created_at         DATETIME DEFAULT NOW                     â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ UNIQUE INDEX: question_hash                                â”‚ â”‚
â”‚  â”‚ INDEX: source                                               â”‚ â”‚
â”‚  â”‚ INDEX: timestamp                                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚               SYSTEM_STATS TABLE                            â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ id                  INTEGER PRIMARY KEY (always 1)          â”‚ â”‚
â”‚  â”‚ total_memories      INTEGER                                 â”‚ â”‚
â”‚  â”‚ auto_saved          INTEGER                                 â”‚ â”‚
â”‚  â”‚ user_corrections    INTEGER                                 â”‚ â”‚
â”‚  â”‚ reuse_count         INTEGER                                 â”‚ â”‚
â”‚  â”‚ groq_responses      INTEGER                                 â”‚ â”‚
â”‚  â”‚ t5_reflections      INTEGER                                 â”‚ â”‚
â”‚  â”‚ last_updated        TEXT                                    â”‚ â”‚
â”‚  â”‚ created_at          DATETIME                                â”‚ â”‚
â”‚  â”‚ updated_at          DATETIME                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SIMILARITY MATCHING ALGORITHM:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Hash incoming question         â”‚
â”‚  2. Fetch recent 50 Q&A pairs      â”‚
â”‚  3. For each stored question:      â”‚
â”‚     a. Sequence similarity (60%)   â”‚
â”‚        - Difflib.SequenceMatcher   â”‚
â”‚     b. Word overlap (40%)          â”‚
â”‚        - Jaccard similarity        â”‚
â”‚  4. Combined score = 0.6a + 0.4b   â”‚
â”‚  5. If score â‰¥ 0.65 â†’ Match        â”‚
â”‚  6. Return best match              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CONFIDENCE SCORING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analyze reflection text for:     â”‚
â”‚                                    â”‚
â”‚  High confidence words:            â”‚
â”‚  - "accurate", "correct"           â”‚
â”‚  - "complete", "comprehensive"     â”‚
â”‚  - "detailed"                      â”‚
â”‚  â†’ Score: 0.8 + (count * 0.05)    â”‚
â”‚                                    â”‚
â”‚  Low confidence words:             â”‚
â”‚  - "unsure", "might", "possibly"   â”‚
â”‚  - "incomplete", "missing"         â”‚
â”‚  â†’ Score: 0.3 - (count * 0.05)    â”‚
â”‚                                    â”‚
â”‚  Default: 0.6                      â”‚
â”‚  User corrections: 1.0             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.3 GROQ Handler Architecture

```python
class LegalGroqLLMHandler:
    """
    Enhanced GROQ handler with legal-specific prompting
    
    Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Legal Question Input               â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  System Prompt Construction         â”‚
    â”‚                                     â”‚
    â”‚  "You are a legal advisor...        â”‚
    â”‚   Provide accurate legal info...    â”‚
    â”‚   DISCLAIMERS:                      â”‚
    â”‚   - General info only               â”‚
    â”‚   - Not legal advice                â”‚
    â”‚   - Consult attorney                â”‚
    â”‚   - Jurisdiction-specific..."       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  API Call to GROQ                   â”‚
    â”‚  Model: openai/gpt-oss-20b          â”‚
    â”‚  Temperature: 0.7                   â”‚
    â”‚  Max Tokens: 768                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Response Processing                â”‚
    â”‚  - Extract content                  â”‚
    â”‚  - Strip whitespace                 â”‚
    â”‚  - Error handling                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Format with Legal Disclaimer       â”‚
    â”‚  (if not already present)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    Legal Answer Output
    """
    
    def generate_legal_answer(self, question: str) -> str:
        system_prompt = """You are a knowledgeable legal advisor assistant. 
        Provide accurate, comprehensive legal information and analysis.
        
        IMPORTANT DISCLAIMERS:
        - Always remind users that this is general legal information, not legal advice
        - Recommend consulting with a qualified attorney for specific legal matters
        - Be thorough but clear in explanations
        - Reference relevant legal principles when applicable
        - If uncertain about jurisdiction-specific laws, mention this limitation
        
        Focus on being helpful while maintaining professional legal standards."""
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Legal Question: {question}"}
            ],
            max_tokens=768,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
```

---

## 4. COMPLETE DATA FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     END-TO-END DATA FLOW                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 1: DATA PREPARATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
constitution_qa.json (Raw)
    â”‚
    â”‚ json_to_csv_formatter.py
    â–¼
formatted_constitution_dataset.csv
    â”‚
    â”‚ preprocess.py (EnhancedDataPreprocessor)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ processed_constitution/           â”‚
â”‚   â”œâ”€ train.csv (70%)              â”‚
â”‚   â”œâ”€ test.csv (20%)               â”‚
â”‚   â”œâ”€ validation.csv (10%)         â”‚
â”‚   â”œâ”€ preprocessing_config.json    â”‚
â”‚   â”œâ”€ preprocessing_stats.json     â”‚
â”‚   â””â”€ preprocessing_report.md      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 2: MODEL TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train.csv + validation.csv
    â”‚
    â”‚ train_legal_model()
    â”‚ - Load Flan-T5-base
    â”‚ - Add legal tokens
    â”‚ - Create LegalQADataset
    â”‚ - Train for 5 epochs
    â”‚ - Save checkpoints
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ flan_t5_legal_advisor_model/      â”‚
â”‚   â”œâ”€ checkpoint-200/              â”‚
â”‚   â”œâ”€ checkpoint-400/              â”‚
â”‚   â”œâ”€ checkpoint-600/ (best)       â”‚
â”‚   â”œâ”€ config.json                  â”‚
â”‚   â”œâ”€ pytorch_model.bin            â”‚
â”‚   â”œâ”€ tokenizer_config.json        â”‚
â”‚   â””â”€ logs/                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 3: RUNTIME INFERENCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Question
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ legal_interactive_session() â”‚
â”‚                             â”‚
â”‚ 1. Check Memory (SQLite)    â”‚
â”‚    â”‚                        â”‚
â”‚    â”œâ”€ Found? â†’ Return       â”‚
â”‚    â”‚                        â”‚
â”‚    â””â”€ Not Found â†“           â”‚
â”‚                             â”‚
â”‚ 2. GROQ Generation          â”‚
â”‚    - LegalGroqLLMHandler    â”‚
â”‚    - System prompt          â”‚
â”‚    - Fast response          â”‚
â”‚    â”‚                        â”‚
â”‚    â–¼                        â”‚
â”‚ 3. T5 Critique              â”‚
â”‚    - Load fine-tuned model  â”‚
â”‚    - Generate critique      â”‚
â”‚    - Identify issues        â”‚
â”‚    â”‚                        â”‚
â”‚    â–¼                        â”‚
â”‚ 4. T5 Improvement           â”‚
â”‚    - Rewrite answer         â”‚
â”‚    - Fix errors             â”‚
â”‚    - Add details            â”‚
â”‚    â”‚                        â”‚
â”‚    â–¼                        â”‚
â”‚ 5. Memory Storage           â”‚
â”‚    - Store in SQLite        â”‚
â”‚    - Update stats           â”‚
â”‚    â”‚                        â”‚
â”‚    â–¼                        â”‚
â”‚ 6. User Feedback            â”‚
â”‚    - Collect corrections    â”‚
â”‚    - Store for retraining   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 4: CONTINUOUS LEARNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Corrections in DB
    â”‚
    â”‚ retrain_from_feedback()
    â”‚ - Extract high-confidence feedback
    â”‚ - Create feedback dataset
    â”‚ - Fine-tune existing model
    â”‚ - Lower learning rate
    â”‚ - Fewer epochs
    â–¼
Updated Model
    â”‚
    â””â”€â†’ Back to Runtime Inference
```

---

## 5. KEY ALGORITHMS IN DEPTH

### 5.1 Similarity Matching Algorithm

```python
def calculate_similarity(question1: str, question2: str) -> float:
    """
    Multi-method similarity calculation
    
    Method 1: Sequence Similarity (60% weight)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Uses Python's difflib.SequenceMatcher
    Compares character-level sequences
    
    Example:
    Q1: "What are fundamental rights?"
    Q2: "What are the fundamental rights?"
    Sequence Similarity = 0.95
    
    
    Method 2: Word Overlap (40% weight)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Jaccard similarity of word sets
    
    Example:
    Q1: "What are fundamental rights?"
    Words1: {what, are, fundamental, rights}
    
    Q2: "What are the fundamental rights?"
    Words2: {what, are, the, fundamental, rights}
    
    Intersection: {what, are, fundamental, rights} = 4
    Union: {what, are, the, fundamental, rights} = 5
    Word Similarity = 4/5 = 0.8
    
    
    Final Score Calculation:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Combined = (0.95 * 0.6) + (0.8 * 0.4)
             = 0.57 + 0.32
             = 0.89
    
    Threshold: 0.65
    Result: MATCH (0.89 â‰¥ 0.65) âœ“
    """
    
    # Normalize
    q1_clean = question1.strip().lower()
    q2_clean = question2.strip().lower()
    
    # Exact match
    if q1_clean == q2_clean:
        return 1.0
    
    # Substring match
    if q1_clean in q2_clean or q2_clean in q1_clean:
        return 0.9
    
    # Sequence similarity (character-level)
    sequence_similarity = difflib.SequenceMatcher(
        None, q1_clean, q2_clean
    ).ratio()
    
    # Word overlap (token-level)
    words1 = set(q1_clean.split())
    words2 = set(q2_clean.split())
    
    if len(words1) == 0 or len(words2) == 0:
        word_similarity = 0.0
    else:
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        word_similarity = intersection / union
    
    # Weighted combination
    combined_similarity = (sequence_similarity * 0.6) + (word_similarity * 0.4)
    
    return combined_similarity
```

### 5.2 Confidence Estimation Algorithm

```python
def estimate_confidence(reflection: str) -> float:
    """
    NLP-based confidence scoring
    
    Keyword Analysis:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    High Confidence Indicators:
    - "accurate", "correct", "complete"
    - "comprehensive", "detailed"
    - "clear", "precise", "exact"
    
    Low Confidence Indicators:
    - "unsure", "might", "possibly"
    - "incomplete", "missing", "unclear"
    - "ambiguous", "uncertain"
    
    Scoring Logic:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Base Score: 0.6
    
    If high_count > low_count:
        score = min(0.8 + (high_count * 0.05), 1.0)
    
    If low_count > high_count:
        score = max(0.3 - (low_count * 0.05), 0.1)
    
    Special Cases:
    - User corrections: Always 1.0
    - Empty reflection: 0.5
    
    Example:
    â”€â”€â”€â”€â”€â”€â”€â”€
    Reflection: "The answer is accurate and comprehensive,
                 though it might be missing some details."
    
    High words found: ["accurate", "comprehensive"] = 2
    Low words found: ["might", "missing"] = 2
    
    high_count == low_count â†’ Base score = 0.6
    """
    
    reflection_lower = reflection.lower()
    
    high_conf_words = [
        "accurate", "correct", "complete", 
        "comprehensive", "detailed"
    ]
    low_conf_words = [
        "unsure", "might", "possibly", 
        "incomplete", "missing", "unclear"
    ]
    
    high_count = sum(1 for word in high_conf_words 
                    if word in reflection_lower)
    low_count = sum(1 for word in low_conf_words 
                   if word in reflection_lower)
    
    if high_count > low_count:
        return min(0.8 + (high_count * 0.05), 1.0)
    elif low_count > high_count:
        return max(0.3 - (low_count * 0.05), 0.1)
    else:
        return 0.6
```

---

## 6. ERROR HANDLING & EDGE CASES

```python
ERROR HANDLING STRATEGIES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. DATA LOADING ERRORS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: CSV format inconsistencies
   Solution: Multi-format detection
   
   if 'question_clean' in df.columns:
       # Already preprocessed
   elif len(df.columns) == 2:
       # Headerless CSV
       df.columns = ['question_clean', 'answer_clean']
   elif 'question' in df.columns:
       # Raw JSON format
       df.rename(columns={'question': 'question_clean'})
   else:
       raise ValueError("Unexpected format")

2. MODEL LOADING ERRORS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: Checkpoint not found
   Solution: Fallback to base model
   
   try:
       checkpoint = get_latest_checkpoint(MODEL_DIR)
       model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
   except Exception:
       print("No checkpoint found, using base model")
       model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_MODEL)

3. GROQ API ERRORS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: API timeout or rate limit
   Solution: Graceful fallback
   
   try:
       response = self.client.chat.completions.create(...)
   except Exception as e:
       return f"API Error: {str(e)}. Please consult an attorney."

4. MEMORY SYSTEM ERRORS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: Database corruption
   Solution: Rebuild from backup
   
   try:
       with self.get_connection() as conn:
           cursor.execute(...)
   except sqlite3.Error:
       print("Database error, attempting recovery")
       self.rebuild_database()

5. TOKENIZATION ERRORS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: Text too long
   Solution: Truncation with warning
   
   if len(text) > MAX_LENGTH:
       print(f"Warning: Text truncated from {len(text)} to {MAX_LENGTH}")
       text = text[:MAX_LENGTH]

6. EMPTY RESPONSE HANDLING
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Problem: Model returns empty string
   Solution: Retry with shorter prompt
   
   def safe_t5(prompt, **kwargs):
       try:
           return t5(prompt, **kwargs)[0]["generated_text"]
       except Exception:
           # Retry with shorter prompt
           short_prompt = " ".join(prompt.split()[:150])
           return t5(short_prompt, **kwargs)[0]["generated_text"]
```

---

# ğŸ“„ README FOR SHOWCASE

I'll create this as a separate, polished README that you can use directly:

---

# ğŸ›ï¸ Legal QA Advisory System

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.30%2B-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> An intelligent legal advisory system that combines Large Language Models with fine-tuned T5 transformers to provide accurate, reflective, and continuously improving legal information on constitutional law.

## ğŸ¯ Overview

The Legal QA Advisory System is a hybrid AI architecture that:
- **Generates** fast initial answers using GROQ LLM
- **Critiques** its own responses using fine-tuned T5
- **Improves** answers through self-reflection
- **Learns** continuously from user feedback
- **Remembers** past consultations for faster responses

### Key Features

- âš¡ **Hybrid Architecture**: Combines GROQ (speed) + T5 (quality)
- ğŸ§  **Self-Reflection**: AI critiques and improves its own answers
- ğŸ’¾ **Memory System**: SQLite-based storage with similarity matching
- ğŸ”„ **Continuous Learning**: Auto-retraining from user corrections
- âš–ï¸ **Legal Focus**: Specialized in constitutional law
- ğŸ”’ **Ethical AI**: Built-in legal disclaimers and limitations

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER QUESTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Memory Check       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€ SQLite Database
    â”‚   (Similarity: 0.65) â”‚           - Past Q&A
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           - Reflections
           â”‚       â”‚                    - Feedback
           â”‚       â””â”€â”€Found â”€â”€â–º Return Cached Answer
           â”‚
           â””â”€â”€Not Found
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   GROQ LLM           â”‚
    â”‚   (Fast Generation)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
         Initial Answer
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   T5 Critique        â”‚
    â”‚   (Find Issues)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
         Reflection
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   T5 Improvement     â”‚
    â”‚   (Enhanced Answer)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Store in Memory    â”‚
    â”‚   + User Feedback    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.8+
PyTorch 2.0+
CUDA (optional, for GPU acceleration)
8GB RAM minimum (16GB recommended)
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/legal-qa-system.git
cd legal-qa-system
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**
```bash
# Create .env file
echo "GROQ_API_KEY=your_api_key_here" > .env
```

4. **Download NLTK data**
```python
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

### Usage

#### 1. Prepare Your Data

Place your legal Q&A dataset in JSON format:

```json
{
  "question": "What are fundamental rights?",
  "answer": "Fundamental rights are basic human rights..."
}
```

#### 2. Run the Complete Pipeline

```bash
python legal_advisor.py
```

#### 3. Menu Options

```
ğŸ›ï¸  LEGAL ADVISOR MAIN MENU  âš–ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Train Legal T5 model
2. Interactive Legal Advisory session
3. Test legal advisory system
4. Show consultation memory
5. Retrain with feedback
6. Database maintenance
7. Export consultation data
8. System statistics
9. Legal disclaimer
10. Exit
```

#### 4. Interactive Session Example

```
ğŸ“ Legal Question: What is due process?

ğŸ” Analyzing legal question with GROQ...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Initial Legal Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Due process is a constitutional principle
that requires fair legal procedures...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš–ï¸ Generating legal critique with T5...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Legal Analysis Critique
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The answer is accurate but could include
more detail on procedural vs substantive
due process...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Enhanced Legal Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Due process requires fair procedures and
protects fundamental rights. It includes:
1. Procedural due process - fair procedures
2. Substantive due process - protection of
   fundamental rights from arbitrary action

âš–ï¸ Disclaimer: This is general legal
information, not legal advice...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¤” Is this analysis helpful? (y/n/correction):
```

---

## ğŸ“Š System Components

### 1. Data Preprocessing

**Module**: `preprocess.py`

```python
from preprocess import EnhancedDataPreprocessor, PreprocessingConfig

config = PreprocessingConfig(
    input_csv_path="constitution_qa.csv",
    text_column="question",
    target_column="answer",
    output_dir="processed_data",
    test_size=0.2,
    validation_size=0.1,
    lemmatize=True,
    remove_stopwords=True
)

preprocessor = EnhancedDataPreprocessor(config)
results = preprocessor.run()
```

**Features**:
- âœ… NLP text cleaning (lemmatization, stopword removal)
- âœ… Duplicate detection (TF-IDF + cosine similarity)
- âœ… Quality assessment and labeling
- âœ… Train/test/validation splitting
- âœ… Comprehensive reporting

### 2. Model Training

**Module**: `legal_advisor.py` â†’ `train_legal_model()`

```python
# Automatically handles:
# - Loading preprocessed data
# - Initializing Flan-T5-base
# - Adding legal-specific tokens
# - Fine-tuning for 5 epochs
# - Checkpoint management
# - Evaluation on validation set

train_legal_model()
```

**Training Details**:
- Base Model: `google/flan-t5-base` (248M parameters)
- Custom Tokens: `<legal_critique>`, `<legal_improve>`, `<statute>`, `<case_law>`
- Epochs: 5
- Batch Size: 2 (CPU-optimized)
- Learning Rate: 5e-5
- Checkpoint Strategy: Save best 3 models

### 3. Hybrid Inference

**Module**: `legal_advisor.py` â†’ `legal_interactive_session()`

**Components**:

a) **GROQ Handler**
```python
groq = LegalGroqLLMHandler(GROQ_API_KEY)
answer = groq.generate_legal_answer(question)
```
- Fast generation (< 2 seconds)
- Legal-specific system prompts
- Automatic disclaimer injection

b) **T5 Critique**
```python
critique_prompt = f"legal_critique: {question}\n{groq_answer}"
reflection = t5(critique_prompt)
```
- Identifies factual errors
- Finds missing information
- Assesses legal accuracy

c) **T5 Improvement**
```python
improve_prompt = f"legal_improve: {critique}\n{original}"
enhanced = t5(improve_prompt)
```
- Fixes identified issues
- Adds missing details
- Improves structure

### 4. Memory System

**Module**: `DatabaseManager` class

**Database Schema**:
```sql
CREATE TABLE feedback (
    id INTEGER PRIMARY KEY,
    question TEXT NOT NULL,
    question_hash TEXT UNIQUE,
    groq_answer TEXT,
    improvement TEXT,
    t5_reflection TEXT,
    source TEXT,
    confidence_score REAL,
    timestamp TEXT
);

CREATE TABLE system_stats (
    total_memories INTEGER,
    auto_saved INTEGER,
    user_corrections INTEGER,
    groq_responses INTEGER,
    t5_reflections INTEGER
);
```

**Features**:
- âœ… Similarity-based retrieval (threshold: 0.65)
- âœ… Confidence scoring (0.0 - 1.0)
- âœ… Usage statistics tracking
- âœ… Automatic cleanup of old entries
- âœ… Export capabilities

---

## ğŸ§ª Testing

### Manual Testing

```bash
# Run interactive session
python legal_advisor.py
# Select option 2: Interactive Legal Advisory session

# Test questions:
1. "What is due process?"
2. "Explain judicial review"
3. "What are fundamental rights?"
```

### System Testing

```bash
# Run test suite
python legal_advisor.py
# Select option 3: Test legal advisory system
```

This will:
- Test GROQ connectivity
- Test T5 model loading
- Test memory system
- Generate sample Q&A

### Database Testing

```bash
# Check memory statistics
python legal_advisor.py
# Select option 4: Show consultation memory
```

---

## ğŸ“ˆ Performance Metrics

### Speed Benchmarks

| Component | Time | Notes |
|-----------|------|-------|
| Memory Check | < 100ms | SQLite query + similarity |
| GROQ Generation | 1-2s | API call |
| T5 Critique | 2-3s | CPU inference |
| T5 Improvement | 3-5s | CPU inference |
| **Total (cold)** | **6-10s** | No memory hit |
| **Total (cached)** | **< 1s** | Memory hit |

### Model Metrics

| Metric | Training | Validation |
|--------|----------|------------|
| Loss | 0.45 | 0.52 |
| Perplexity | 1.57 | 1.68 |
| BLEU Score | 0.68 | 0.63 |

### Memory Statistics

```
Total Consultations: 150
Auto-saved: 120
User Corrections: 30
Memory Reuse Rate: 42%
Average Confidence: 0.78
```

---

## ğŸ—‚ï¸ Project Structure

```
legal-qa-system/
â”‚
â”œâ”€â”€ Dev/
â”‚   â”œâ”€â”€ preprocessed_data/
â”‚   â”‚   â”œâ”€â”€ processed_constitution/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â”‚   â””â”€â”€ validation.csv
â”‚   â”‚   â””â”€â”€ formatted_constitution_dataset.csv
â”‚   â”‚
â”‚   â””â”€â”€ flan_t5_legal_advisor_model/
â”‚       â”œâ”€â”€ checkpoint-600/ (best model)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ legal_feedback_system.db
â”‚       â””â”€â”€ logs/
â”‚
â”œâ”€â”€ legal_advisor.py           # Main system (Document 4)
â”œâ”€â”€ preprocess.py              # Data preprocessing
â”œâ”€â”€ json_to_csv_formatter.py  # Data formatter
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                       # API keys
â””â”€â”€ README.md                  # This file
```

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# .env file
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=openai/gpt-oss-20b  # Optional, default shown
```

### System Configuration

```python
# In legal_advisor.py

# Paths
PREPROCESSED_DIR = "Dev/preprocessed_data/processed_constitution"
MODEL_DIR = "Dev/flan_t5_legal_advisor_model"
DATABASE_PATH = f"{MODEL_DIR}/legal_feedback_system.db"

# Model Parameters
PRETRAINED_MODEL = "google/flan-t5-base"
MAX_INPUT_LENGTH = 768
MAX_TARGET_LENGTH = 768
BATCH_SIZE = 2
NUM_TRAIN_EPOCHS = 5

# Memory Settings
SIMILARITY_THRESHOLD = 0.65
```

---

## ğŸ“š Dependencies

### Core Libraries

```txt
# requirements.txt

torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
accelerate>=0.20.0

# NLP
nltk>=3.8
scikit-learn>=1.2.0

# Data Processing
pandas>=2.0.0
numpy>=1.24.0

# Database
sqlite3  # Built-in

# API
groq>=0.4.0
python-dotenv>=1.0.0

# UI
rich>=13.0.0
```

### Installation Command

```bash
pip install torch transformers datasets accelerate nltk scikit-learn pandas numpy groq python-dotenv rich
```

---

## ğŸ“ Educational Value

### Key Concepts Demonstrated

1. **Hybrid AI Architecture**
   - Combining multiple AI models
   - Leveraging strengths of each
   - Efficient resource usage

2. **Self-Reflection in AI**
   - Meta-learning concepts
   - Self-critique mechanisms
   - Iterative improvement

3. **NLP Pipeline**
   - Text preprocessing
   - Tokenization
   - Lemmatization
   - POS tagging

4. **Transfer Learning**
   - Fine-tuning pre-trained models
   - Domain adaptation
   - Catastrophic forgetting prevention

5. **Production ML**
   - Model versioning
   - Checkpoint management
   - Memory optimization
   - Error handling

6. **Database Design**
   - Schema design
   - Indexing strategies
   - Query optimization

---

## ğŸš§ Limitations & Future Work

### Current Limitations

1. **Domain Specificity**
   - Trained only on constitutional law
   - May not generalize to other legal areas

2. **Language Support**
   - English only currently
   - No multilingual support

3. **Jurisdiction**
   - US-centric constitutional knowledge
   - Limited international law coverage

4. **Compute Requirements**
   - CPU inference is slow (3-5s per response)
   - GPU strongly recommended for production

5. **Scale**
   - SQLite limitations for very large deployments
   - No distributed training support

### Planned Improvements

- [ ] Add GPU acceleration
- [ ] Implement RAG (Retrieval Augmented Generation)
- [ ] Add citation system for legal sources
- [ ] Multi-jurisdictional support
- [ ] Domain validation layer
- [ ] Automated testing suite
- [ ] Web UI interface
- [ ] API endpoints
- [ ] Docker containerization
- [ ] Monitoring and analytics dashboard

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup

```bash
# Fork and clone
git clone https://github.com/yourusername/legal-qa-system.git
cd legal-qa-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e .

# Run tests
pytest tests/
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## âš ï¸ Legal Disclaimer

This system provides general legal information for educational purposes only. It does NOT provide legal advice and cannot replace consultation with a qualified attorney.

- Responses are for informational purposes only
- Laws vary by jurisdiction and change frequently
- Specific legal matters require professional legal counsel
- No attorney-client relationship is created
- Always verify information with current legal sources

**For legal advice specific to your situation, consult a licensed attorney.**

---

## ğŸ‘¨â€ğŸ’» Author

**Kaustubh**


---

## ğŸ™ Acknowledgments

- **Flan-T5**: Google Research
- **GROQ**: GROQ Inc.
- **Transformers Library**: Hugging Face
- **Constitutional Dataset**: [Source]
- **Inspiration**: Reflective QA systems research

---


---

**Made with âš–ï¸ and ğŸ¤– by Kaustubh**

---

